import requests
from urllib.parse import quote
from bs4 import BeautifulSoup
from openai import OpenAI
import os
from modules.logging_config import logger

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") 

client = OpenAI(api_key=OPENAI_API_KEY)


def r√©sum√©(texte_article, entreprise_nom):
    # Tronquage si le texte est trop long (garde les 10 000 premiers caract√®res)
    texte_article = texte_article[:10000] + "\n[Texte tronqu√©]" if len(texte_article) > 10000 else texte_article

    messages = [
        {
            "role": "system",
            "content": "Tu es un assistant professionnel et p√©dagogue."
        },
        {
            "role": "user",
            "content": f"Voici un article Wikip√©dia. S'il est bien √† propos de l'entreprise {entreprise_nom}, r√©sume-le en 1000 signes maximum. Sinon, renvoie EXACTEMENT la phrase 'Voici votre brief !'. Voici l'article Wikip√©dia :\n\n{texte_article}"
        }
    ]

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            temperature=0  
        )
        # V√©rifie que la r√©ponse est bien form√©e
        output = response.choices[0].message.content.strip()
        if not output:
            logger.warning("R√©ponse vide wikip√©dia.")
            return None
        return output
    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration du r√©sum√© : {e}")
        return None


def get_wikipedia_text(entreprise_nom, lang="fr"):
    """
    Recherche le bon article Wikip√©dia pour une entreprise et retourne le texte brut.
    """

    headers = {
        "User-Agent": "MyCompanyApp/1.0 (contact@mycompany.com)"
    }

    # üîé √âtape 1 : rechercher le titre exact
    search_url = f"https://{lang}.wikipedia.org/w/api.php"
    params = {
        "action": "query",
        "list": "search",
        "srsearch": entreprise_nom,
        "format": "json"
    }

    try:
        search_resp = requests.get(search_url, params=params, headers=headers)
        search_resp.raise_for_status()
    except Exception as e:
        print("‚ùå Erreur lors de la recherche Wikip√©dia :", e)
        return None

    search_results = search_resp.json()
    if not search_results.get("query", {}).get("search"):
        print("‚ùå Aucun r√©sultat trouv√© sur Wikip√©dia.")
        return None

    # üéØ Prendre le premier r√©sultat
    page_title = search_results["query"]["search"][0]["title"]
    encoded_title = quote(page_title)

    # üìÑ √âtape 2 : r√©cup√©ration du contenu HTML
    html_url = f"https://{lang}.wikipedia.org/api/rest_v1/page/html/{encoded_title}"

    try:
        html_resp = requests.get(html_url, headers=headers)
        html_resp.raise_for_status()
    except Exception as e:
        print(f"‚ùå Impossible de r√©cup√©rer l'article : {e}")
        return None

    soup = BeautifulSoup(html_resp.text, "html.parser")

    # üßπ Nettoyage du contenu
    for tag in soup(["table", "style", "script", "noscript", "img", "figure", "nav"]):
        tag.decompose()

    text = soup.get_text(separator="\n", strip=True)

    # ‚ö†Ô∏è V√©rifie si c'est une page d'homonymie
    if "Ceci est une page d‚Äôhomonymie" in text.lower():
        print("‚ö†Ô∏è Page de d√©sambigu√Øsation d√©tect√©e.")
        return None

    return text


def wikipedia_resume(entreprise_nom):
    texte_article = get_wikipedia_text(entreprise_nom)
    if texte_article:
        print("article wikip√©dia trouv√©")
        logger.info("article wikip√©dia trouv√©")
        return r√©sum√©(texte_article, entreprise_nom)
    else:
        print("Aucun texte r√©cup√©r√© ou probl√®me d'article Wikip√©dia.")
        logger.info("Aucun texte r√©cup√©r√© ou probl√®me d'article Wikip√©dia.")
        return None



